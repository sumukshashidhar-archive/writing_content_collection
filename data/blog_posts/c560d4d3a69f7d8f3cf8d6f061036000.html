Title: The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions

URL Source: https://openai.com/index/the-instruction-hierarchy/

Markdown Content:
The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions | OpenAI
===============

[Skip to main content](https://openai.com/index/the-instruction-hierarchy/#main)

[](https://openai.com/)

*   Research
*   Products
*   Safety
*   Company

*   [Abstract](https://openai.com/index/the-instruction-hierarchy/#_5UT7om6NlHJ8zkBbMkiTVI)

April 19, 2024 April 19, 2024

The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
==============================================================================

[Read paper(opens in a new window)](https://arxiv.org/abs/2404.13208)

![Image 1: Media: The Instruction Hierarchy](https://images.ctfassets.net/kftzwdyauwt9/7KETXhKfus6Y3CS6ON6HNQ/745ec415ffc13f16563cde91a98f8e30/image_44.png?w=3840&q=90&fm=webp)

Abstract
--------

Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.

*   [Research](https://openai.com/news/research/?tags=topic-research)

Authors
-------

[Eric Wallace(opens in a new window)](https://arxiv.org/search/cs?searchtype=author&query=Wallace,+E)

[Kai Xiao(opens in a new window)](https://arxiv.org/search/cs?searchtype=author&query=Xiao,+K)

[Reimar Leike(opens in a new window)](https://arxiv.org/search/cs?searchtype=author&query=Leike,+R)

[Lilian Weng(opens in a new window)](https://arxiv.org/search/cs?searchtype=author&query=Weng,+L)

[Johannes Heidecke(opens in a new window)](https://arxiv.org/search/cs?searchtype=author&query=Heidecke,+J)

[Alex Beutel(opens in a new window)](https://arxiv.org/search/cs?searchtype=author&query=Beutel,+A)

Our research

*   [Overview](https://openai.com/research/)
*   [Index](https://openai.com/news/research/)

Latest advancements

*   [GPT-4](https://openai.com/index/gpt-4/)
*   [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
*   [DALL·E 3](https://openai.com/index/dall-e-3/)
*   [Sora](https://openai.com/index/sora/)

ChatGPT

*   [For Everyone](https://openai.com/chatgpt/)
*   [For Teams](https://openai.com/chatgpt/team/)
*   [For Enterprises](https://openai.com/chatgpt/enterprise/)
*   [ChatGPT login(opens in a new window)](https://chatgpt.com/)
*   [Download](https://openai.com/chatgpt/download/)

API

*   [Platform overview](https://openai.com/api/)
*   [Pricing](https://openai.com/api/pricing/)
*   [Documentation(opens in a new window)](https://platform.openai.com/docs/introduction)
*   [API login(opens in a new window)](https://platform.openai.com/login?launch)

Explore more

*   [OpenAI for business](https://openai.com/business/)
*   [Stories](https://openai.com/news/stories/)

Safety overview

*   [Safety overview](https://openai.com/safety/)
*   [Safety standards](https://openai.com/safety-standards/)

Teams

*   [Safety Systems](https://openai.com/safety-systems/)
*   [Preparedness](https://openai.com/preparedness/)
*   [Superalignment](https://openai.com/superalignment/)

Company

*   [About us](https://openai.com/about/)
*   [News](https://openai.com/news/)
*   [Our Charter](https://openai.com/charter/)
*   [Security](https://openai.com/security-and-privacy/)
*   [Residency](https://openai.com/residency/)
*   [Careers](https://openai.com/careers/)

Terms & policies

*   [Terms of use](https://openai.com/policies/terms-of-use/)
*   [Privacy policy](https://openai.com/policies/privacy-policy/)
*   [Brand guidelines](https://openai.com/brand/)
*   [Other policies](https://openai.com/policies/)

OpenAI © 2015–2024

[(opens in a new window)](https://x.com/OpenAI)[(opens in a new window)](https://www.youtube.com/OpenAI)[(opens in a new window)](https://www.linkedin.com/company/openai)[(opens in a new window)](https://github.com/openai)[(opens in a new window)](https://www.instagram.com/openai/?hl=en)[(opens in a new window)](https://www.tiktok.com/@openai?lang=en)[(opens in a new window)](https://discord.gg/openai)
