\section{Introduction}

\thispagestyle{empty}


Consistency models \citep{song2023consistency} are an emerging family of generative models that produce high-quality samples using a single network evaluation. Unlike GANs \citep{goodfellow2014generative}, consistency models are not trained with adversarial optimization and thus sidestep the associated training difficulty. Compared to score-based diffusion models \citep{sohl2015deep,song2019generative,song2020improved,ho2020denoising,song2021scorebased}, consistency models do not require numerous sampling steps to generate high-quality samples. They are trained to generate samples in a single step, but still retain important advantages of diffusion models, such as the flexibility to exchange compute for sample quality through multistep sampling, and the ability to perform zero-shot data editing.

We can train consistency models using either consistency distillation (CD) or consistency training (CT). The former requires pre-training a diffusion model and distilling the knowledge therein into a consistency model. The latter allows us to train consistency models directly from data, establishing them as an independent family of generative models. Previous work \citep{song2023consistency} demonstrates that CD significantly outperforms CT. However, CD adds computational overhead to the training process since it requires learning a separate diffusion model. Additionally, distillation limits the sample quality of the consistency model to that of the diffusion model. To avoid the downsides of CD and to position consistency models as an independent family of generative models, we aim to improve CT to either match or exceed the performance of CD.

For optimal sample quality, both CD and CT rely on learned metrics like the Learned Perceptual Image Patch Similarity (LPIPS) \citep{zhang2018unreasonable} in previous work \citep{song2023consistency}. However, depending on LPIPS has two primary downsides. Firstly, there could be potential bias in evaluation since the same ImageNet dataset \citep{deng2009imagenet} trains both LPIPS and the Inception network in Fr\'echet Inception Distance (FID) \citep{heusel2017gans}, which is the predominant metric for image quality. As analyzed in \citet{kynkanniemi2023the}, improvements of FIDs can come from accidental leakage of ImageNet features from LPIPS, causing inflated FID scores. Secondly, learned metrics require pre-training auxiliary networks for feature extraction. Training with these metrics requires backpropagating through extra neural networks, which increases the demand for compute.

To tackle these challenges, we introduce improved techniques for CT that not only surpass CD in sample quality but also eliminate the dependence on learned metrics like LPIPS. Our techniques are motivated from both theoretical analysis, and comprehensive experiments on the CIFAR-10 dataset \citep{krizhevsky2014cifar}. Specifically, we perform an in-depth study on the empirical impact of weighting functions, noise embeddings, and dropout in CT. Additionally, we identify an overlooked flaw in prior theoretical analysis for CT and propose a simple fix by removing the Exponential Moving Average (EMA) from the teacher network. We adopt Pseudo-Huber losses from robust statistics to replace LPIPS. Furthermore, we study how sample quality improves as the number of discretization steps increases, and utilize the insights to propose a simple but effective curriculum for total discretization steps. Finally, we propose a new schedule for sampling noise levels in the CT objective based on lognormal distributions.

Taken together, these techniques allow CT to attain FID scores of 2.51 and 3.25 for CIFAR-10 and ImageNet $64\times 64$ in one sampling step, respectively. These scores not only surpass CD but also represent improvements of 3.5$\times$ and 4$\times$ over previous CT methods. Furthermore, they significantly outperform the best few-step diffusion distillation techniques for diffusion models even without the need for distillation. By two-step generation, we achieve improved FID scores of 2.24 and 2.77 on CIFAR-10 and ImageNet $64\times 64$, surpassing the scores from CD in both one-step and two-step settings. These results rival many top-tier diffusion models and GANs, showcasing the strong promise of consistency models as a new independent family of generative models.
