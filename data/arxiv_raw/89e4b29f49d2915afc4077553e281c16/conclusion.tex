\section{Conclusion}
Our improved techniques for CT have successfully addressed its previous limitations, surpassing the performance of CD in generating high-quality samples without relying on LPIPS. We examined the impact of weighting functions, noise embeddings, and dropout. By removing EMA for teacher networks, adopting Pseudo-Huber losses in lieu of LPIPS, combined with a new curriculum for discretization and noise sampling schedule, we have achieved unprecedented FID scores for consistency models on both CIFAR-10 and ImageNet $64\times 64$ datasets. Remarkably, these results outpace previous CT methods by a considerable margin, surpass previous few-step diffusion distillation techniques, and challenge the sample quality of leading diffusion models and GANs.

\section*{Acknowledgements}
We would like to thank Alex Nichol, Allan Jabri, Ishaan Gulrajani, and Jakub Pachocki for insightful technical discussions. We also appreciate Mark Chen and Ilya Sutskever for their unwavering support throughout this project.







