\section{Consistency models}\label{sec:background}

Central to the formulation of consistency models is the probability flow ordinary differential equation (ODE) from \citet{song2021scorebased}. Let us denote the data distribution by $p_\text{data}(\rvx)$. When we add Gaussian noise with mean zero and standard deviation $\sigma$ to this data, the resulting perturbed distribution is given by $p_\sigma(\rvx) = \int p_\text{data}(\rvy) \mcal{N}(\rvx \mid \rvy, \sigma^2 \mI) \ud \rvy$. The probability flow ODE, as presented in \citet{Karras2022edm}, takes the form of
\begin{align}
    \frac{\ud \rvx}{\ud \sigma} = -\sigma \nabla_\rvx \log p_{\sigma}(\rvx)\quad \sigma \in[\sigma_\text{min}, \sigma_\text{max}],\label{eq:prob-flow-ode}
\end{align}
where the term $\nabla_\rvx \log p_{\sigma}(\rvx)$ is known as the \emph{score function} of $p_{\sigma}(\rvx)$ \citep{song2019sliced,song2019generative,song2020improved,song2021scorebased}. Here $\sigma_\text{min}$ is a small positive value such that $p_{\sigma_\text{min}}(\rvx) \approx p_\text{data}(\rvx)$, introduced to avoid numerical issues in ODE solving. Meanwhile, $\sigma_\text{max}$ is sufficiently large so that $p_{\sigma}(\rvx) \approx \mathcal{N}(\bm{0}, \sigma_\text{max}^2 \mathbf{I})$. Following \citet{Karras2022edm,song2023consistency}, we adopt $\sigma_\text{min} = 0.002$, and $\sigma_\text{max} = 80$ throughout the paper. Crucially, solving the probability flow ODE from noise level $\sigma_1$ to $\sigma_2$ allows us to transform a sample $\rvx_{\sigma_1} \sim p_{\sigma_1}(\rvx)$ into $\rvx_{\sigma_2} \sim p_{\sigma_2}(\rvx)$.


The ODE in \cref{eq:prob-flow-ode} establishes a bijective mapping between a noisy data sample $\rvx_\sigma \sim p_{\sigma}(\rvx)$ and $\rvx_{\sigma_\text{min}} \sim p_{\sigma_\text{min}}(\rvx) \approx p_\text{data}(\rvx)$. This mapping, denoted as $\vf^*: (\rvx_\sigma, \sigma) \mapsto \rvx_{\sigma_\text{min}}$, is termed the \emph{consistency function}. By its very definition, the consistency function satisfies the \emph{boundary condition} $\vf^*(\rvx, \sigma_\text{min}) = \rvx$. A \emph{consistency model}, which we denote by $\vf_\vtheta(\rvx, \sigma)$, is a neural network trained to approximate the consistency function $\vf^*(\rvx, \sigma)$. To meet the boundary condition, we follow \citet{song2023consistency} to parameterize the consistency model as
\begin{align}
\vf_\vtheta(\rvx, \sigma) = c_\text{skip}(\sigma) \rvx + c_\text{out}(\sigma) \bm{F}_\vtheta(\rvx, \sigma),
\end{align}
where $\bm{F}_\vtheta(\rvx, \sigma)$ is a free-form neural network, while $c_\text{skip}(\sigma)$ and $c_\text{out}(\sigma)$ are differentiable functions such that $c_\text{skip}(\sigma_\text{min}) = 1$ and $c_\text{out}(\sigma_\text{min}) = 0$.


To train the consistency model, we discretize the probability flow ODE using a sequence of noise levels $\sigma_\text{min} = \sigma_1 < \sigma_2 < \cdots < \sigma_N = \sigma_\text{max}$, where we follow \citet{Karras2022edm,song2023consistency} in setting $\sigma_i = (\sigma_\text{min}^{1/\rho} + \frac{i-1}{N - 1}(\sigma_\text{max}^{1/\rho} - \sigma_\text{min}^{1/\rho}))^\rho$ for $i \in \llbracket 1, N \rrbracket$, and $\rho = 7$, where $\llbracket a, b \rrbracket$ denotes the set of integers $\{a, a+1, \cdots, b\}$. The model is trained by minimizing the following \emph{consistency matching} (CM) loss over $\vtheta$:
\begin{align}
\mcal{L}^N(\vtheta, \vtheta^{-}) = \mbb{E}\left[ \lambda(\sigma_i) d(\vf_\vtheta(\rvx_{\sigma_{i+1}}, \sigma_{i+1}), \vf_{\vtheta^{-}}(\breve{\rvx}_{\sigma_i}, \sigma_i)) \right],\label{eq:cm-loss}
\end{align}
where $\breve{\rvx}_{\sigma_i} = \rvx_{\sigma_{i+1}} - (\sigma_i - \sigma_{i+1})\sigma_{i+1} \nabla_\rvx \log p_{\sigma_{i+1}}(\rvx)|_{\rvx = \rvx_{\sigma_{i+1}}}$. In \cref{eq:cm-loss}, $d(\vx, \vy)$ is a metric function comparing vectors $\vx$ and $\vy$, and $\lambda(\sigma) > 0$ is a weighting function. Typical metric functions include the squared $\ell_2$ metric $d(\vx, \vy) = \norm{\vx-\vy}_2^2$, and the Learned Perceptual Image Patch Similarity (LPIPS) metric introduced in \citet{zhang2018unreasonable}. The expectation in \cref{eq:cm-loss} is taken over the following sampling process: $i \sim \mcal{U}\llbracket 1, N - 1 \rrbracket$ where $\mcal{U}\llbracket 1, N-1\rrbracket$ represents the uniform distribution over $\{1,2,\cdots, N-1\}$, and $\rvx_{\sigma_{i+1}} \sim p_{\sigma_{i+1}}(\rvx)$. Note that $\breve{\rvx}_{\sigma_{i}}$ is derived from $\rvx_{\sigma_{i+1}}$ by solving the probability flow ODE in the reverse direction for a single step. In \cref{eq:cm-loss}, $\vf_{\vtheta}$ and $\vf_{\vtheta^{-}}$ are referred to as the \emph{student network} and the \emph{teacher network}, respectively. The teacher's parameter $\vtheta^{-}$ is obtained by applying Exponential Moving Average (EMA) to the student's parameter $\vtheta$ during the course of training as follows:
\begin{align}
\vtheta^{-} \gets \operatorname{stopgrad}(\mu \vtheta^{-} + (1 - \mu) \vtheta),\label{eq:ema}
\end{align}
with $0 \leq \mu < 1$ representing the EMA decay rate. Here we explicitly employ the $\operatorname{stopgrad}$ operator to highlight that the teacher network remains fixed for each optimization step of the student network. However, in subsequent discussions, we will omit the $\operatorname{stopgrad}$ operator when its presence is clear and unambiguous. In practice, we also maintain EMA parameters for the student network to achieve better sample quality at inference time. It is clear that as $N$ increases, the consistency model optimized using \cref{eq:cm-loss} approaches the true consistency function. For faster training, \citet{song2023consistency} propose a curriculum learning strategy where $N$ is progressively increased and the EMA decay rate $\mu$ is adjusted accordingly. This curriculum for $N$ and $\mu$ is denoted by $N(k)$ and $\mu(k)$, where $k \in \mbb{N}$ is a non-negative integer indicating the current training step.

Given that $\breve{\rvx}_{\sigma_i}$ relies on the unknown score function $\nabla_\rvx \log p_{\sigma_{i+1}}(\rvx)$, directly optimizing the consistency matching objective in \cref{eq:cm-loss} is infeasible. To circumvent this challenge, \citet{song2023consistency} propose two training algorithms: \emph{consistency distillation} (CD) and \emph{consistency training} (CT). For consistency distillation, we first train a diffusion model $\vs_\vphi(\rvx, \sigma)$ to estimate $\nabla_\rvx \log p_\sigma(\rvx)$ via score matching \citep{hyvarinen-EstimationNonNormalizedStatistical-2005,vincent2011connection,song2019sliced,song2019generative}, then approximate $\breve{\rvx}_{\sigma_i}$ with $\hat{\rvx}_{\sigma_i} = \rvx_{\sigma_{i+1}} - (\sigma_i - \sigma_{i+1})\sigma_{i+1} \vs_\vphi(\rvx_{\sigma_{i+1}}, \sigma_{i+1})$. On the other hand, consistency training employs a different approximation method. Recall that $\rvx_{\sigma_{i+1}} = \rvx + \sigma_{i+1} \rvz$ with $\rvx \sim p_\text{data}(\rvx)$ and $\rvz \sim \mcal{N}(\bm{0}, \mI)$. Using the same $\rvx$ and $\rvz$, \citet{song2023consistency} define $\check{\rvx}_{\sigma_i} = \rvx + \sigma_i \rvz$ as an approximation to $\breve{\rvx}_{\sigma_i}$, which leads to the consistency training objective below:
\begin{align}
\mcal{L}^N_\text{CT}(\vtheta, \vtheta^{-}) = \mbb{E}\left[ \lambda(\sigma_i) d(\vf_\vtheta(\rvx + \sigma_{i+1}\rvz, \sigma_{i+1}), \vf_{\vtheta^{-}}(\rvx + \sigma_i \rvz, \sigma_i)) \right].\label{eq:ct-loss}
\end{align}
As analyzed in \citet{song2023consistency}, this objective is asymptotically equivalent to consistency matching in the limit of $N\to\infty$. We will revisit this analysis in \cref{sec:noema}.


After training a consistency model $\vf_\vtheta(\rvx, \sigma)$ through CD or CT, we can directly generate a sample $\rvx$ by starting with $\rvz \sim \mcal{N}(\bm{0}, \sigma_\text{max}^2\mI)$ and computing $\rvx = \vf_\vtheta(\rvz, \sigma_\text{max})$. Notably, these models also enable multistep generation. For a sequence of indices $1 = i_1 < i_2 < \cdots < i_K = N$, we start by sampling $\rvx_{K} \sim \mcal{N}(\bm{0}, \sigma_\text{max}^2 \mI)$ and then iteratively compute $\rvx_{k} \gets \vf_\vtheta(\rvx_{k+1}, \sigma_{i_{k+1}}) + \sqrt{\sigma_{i_k}^2 - \sigma_\text{min}^2} \rvz_{k}$ for $k = K-1, K-2, \cdots, 1$, where $\rvz_{k} \sim \mcal{N}(\bm{0}, \mI)$. The resulting sample $\rvx_{1}$ approximates the distribution $p_\text{data}(\rvx)$. In our experiments, setting $K=3$ (two-step generation) often enhances the quality of one-step generation considerably, though increasing the number of sampling steps further provides diminishing benefits.



