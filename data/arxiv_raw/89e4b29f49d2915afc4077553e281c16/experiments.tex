\section{Putting it together}
\begin{table*}
    \begin{minipage}[t]{0.49\linewidth}
	\caption{Comparing the quality of unconditional samples on CIFAR-10.}\label{tab:cifar-10}
	\centering
	{\setlength{\extrarowheight}{1.5pt}
	\begin{adjustbox}{max width=\linewidth}
	\begin{tabular}{@{}l@{\hspace{-0.2em}}c@{\hspace{0.3em}}c@{\hspace{0.3em}}c@{}}
        \Xhline{3\arrayrulewidth}
	    METHOD & NFE ($\downarrow$) & FID ($\downarrow$) & IS ($\uparrow$) \\
        \\[-2ex]
        \multicolumn{4}{@{}l}{\textbf{Fast samplers \& distillation for diffusion models}}\\\Xhline{3\arrayrulewidth}
        DDIM \citep{song2020denoising} & 10 & 13.36 &\\
        DPM-solver-fast \citep{lu2022dpm} & 10 & 4.70 & \\
        3-DEIS \citep{zhang2022fast} & 10 & 4.17 & \\
        UniPC \citep{zhao2023unipc} & 10 & 3.87 & \\
        Knowledge Distillation \citep{luhman2021knowledge} & 1 & 9.36 &  \\
        DFNO (LPIPS) \citep{zheng2022fast} & 1 & 3.78 & \\
        2-Rectified Flow (+distill) \citep{liu2022flow}
         & 1 & 4.85 & 9.01\\
        TRACT \citep{berthelot2023tract} & 1 & 3.78 & \\
         & 2 & 3.32 & \\
        Diff-Instruct \citep{luo2023diff} & 1 & 4.53 & 9.89\\
        PD$^*$ \citep{salimans2022progressive} & 1 & 8.34 & 8.69 \\
          & 2 & 5.58 & 9.05 \\
        CD (LPIPS) \citep{song2023consistency} & 1 & 3.55 & 9.48 \\
          & 2 & 2.93 & 9.75 \\
        \multicolumn{4}{@{}l}{\textbf{Direct Generation}}\\\Xhline{3\arrayrulewidth}
        Score SDE \citep{song2021scorebased} & 2000 & 2.38 & 9.83\\
        Score SDE (deep) \citep{song2021scorebased} & 2000 & 2.20 & 9.89\\
        DDPM \citep{ho2020denoising} & 1000 & 3.17 & 9.46\\
        LSGM \citep{vahdat2021score} & 147 & 2.10 &\\
        PFGM \citep{xu2022poisson} & 110 & 2.35 & 9.68 \\
        EDM$^*$ \citep{Karras2022edm}
         & 35 & 2.04 & 9.84 \\
        EDM-G++ \citep{kim2023refining} & 35 & 1.77 & \\
        IGEBM \citep{duimplicit2019} & 60 & 40.6 & 6.02 \\
        NVAE \citep{vahdat2020nvae} & 1 & 23.5 & 7.18\\
        Glow \citep{kingma2018glow}
         & 1 & 48.9 & 3.92 \\
        Residual Flow \citep{chen2019residual} & 1 & 46.4\\
        BigGAN \citep{brock2018large} & 1 & 14.7 & 9.22\\
        StyleGAN2 \citep{karras2020analyzing}
         & 1 & 8.32 & 9.21\\
        StyleGAN2-ADA \citep{karras2020training}
         & 1 & 2.92 & 9.83\\
        CT (LPIPS) \citep{song2023consistency} & 1 & 8.70 & 8.49 \\
          & 2 & 5.83 & 8.85 \\
        \textbf{iCT (ours)} & 1 & 2.83 & 9.54 \\
        & 2 & 2.46 & 9.80 \\
        \textbf{iCT-deep (ours)} & 1 & 2.51 & 9.76 \\
        & 2 & 2.24 & 9.89
	\end{tabular}
    \end{adjustbox}
	}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\linewidth}
    \caption{Comparing the quality of class-conditional samples on ImageNet $64\times 64$.}\label{tab:imagenet-64}
    \centering
    {\setlength{\extrarowheight}{1.5pt}
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{@{}l@{\hspace{0.2em}}c@{\hspace{0.3em}}c@{\hspace{0.3em}}c@{\hspace{0.3em}}c@{}}
        \Xhline{3\arrayrulewidth}
        METHOD & NFE ($\downarrow$) & FID ($\downarrow$) & Prec. ($\uparrow$) & Rec. ($\uparrow$) \\
        \\[-2ex]
        \multicolumn{4}{@{}l}{\textbf{Fast samplers \& distillation for diffusion models}}\\\Xhline{3\arrayrulewidth}
        DDIM \citep{song2020denoising} & 50 & 13.7 & 0.65 & 0.56\\
        & 10 & 18.3 & 0.60 & 0.49\\
        DPM solver \citep{lu2022dpm} & 10 & 7.93 &&\\
        & 20 & 3.42 &&\\
        DEIS \citep{zhang2022fast} & 10 & 6.65 & \\
        & 20 & 3.10 && \\
        DFNO (LPIPS) \citep{zheng2022fast} & 1 & 7.83 & & 0.61\\
        TRACT \citep{berthelot2023tract} & 1 & 7.43 & & \\
            & 2 & 4.97 & & \\
        BOOT \citep{gu2023boot} & 1 & 16.3 & 0.68 & 0.36\\
        Diff-Instruct \citep{luo2023diff} & 1 & 5.57 & &\\
        PD$^*$ \citep{salimans2022progressive} & 1 & 15.39 & 0.59 & 0.62 \\
            & 2 & 8.95 & 0.63 & 0.65 \\
            & 4 & 6.77 & 0.66 & 0.65 \\
        PD (LPIPS) \citep{song2023consistency} & 1 & 7.88 & 0.66 & 0.63 \\
            & 2 & 5.74 & 0.67 & 0.65 \\
            & 4 & 4.92 & 0.68 & 0.65 \\
        CD (LPIPS) \citep{song2023consistency} & 1 & 6.20 & 0.68 & 0.63 \\
            & 2 & 4.70 & 0.69 & 0.64 \\
            & 3 & 4.32 & 0.70 & 0.64 \\
        \multicolumn{4}{@{}l}{\textbf{Direct Generation}}\\\Xhline{3\arrayrulewidth}
        RIN \citep{jabri2023rin} & 1000 & 1.23 & & \\
        DDPM \citep{ho2020denoising} & 250 & 11.0 & 0.67 & 0.58 \\
        iDDPM \citep{nichol2021improved} & 250 & 2.92 & 0.74 & 0.62\\
        ADM \citep{dhariwal2021diffusion} & 250 & 2.07 & 0.74 & 0.63\\
        EDM \citep{Karras2022edm}
            & 511 & 1.36 & & \\
        EDM$^*$ (Heun) \citep{Karras2022edm} & 79 & 2.44 & 0.71 & 0.67\\
        BigGAN-deep \citep{brock2018large} & 1 & 4.06 & 0.79 & 0.48\\
        CT (LPIPS) \citep{song2023consistency} & 1 & 13.0 & 0.71 & 0.47 \\
            & 2 & 11.1 & 0.69 & 0.56\\
        \textbf{iCT (ours)} & 1 & 4.02 & 0.70 & 0.63\\
        & 2 & 3.20 & 0.73 & 0.63 \\
        \textbf{iCT-deep (ours)} & 1 & 3.25 & 0.72 & 0.63 \\
        & 2 & 2.77 & 0.74 & 0.62
    \end{tabular}
    \end{adjustbox}
    }
\end{minipage}
\vspace{-1em}
\captionsetup{labelformat=empty, labelsep=none, font=scriptsize}
\caption{Most results for existing methods are taken from a previous paper, except for those marked with *, which are from our own re-implementation.}
\end{table*}

\begin{figure}
    \centering
    \begin{minipage}[t]{0.33\textwidth}
        \vspace{-24.5em}
        \begin{subfigure}[b]{\linewidth}
            \centering
            \includegraphics[width=\linewidth]{figures/cifar10_mid.png}
            \caption{One-step samples on CIFAR-10.}
        \end{subfigure}\\
        \begin{subfigure}[b]{\linewidth}
            \centering
            \vspace{1.5em}
            \includegraphics[width=\linewidth]{figures/cifar10_2_step_mid.png}
            \caption{Two-step samples on CIFAR-10.}
        \end{subfigure}
    \end{minipage}\hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/in64_deep_mid.png}
        \caption{One-step samples on ImageNet.}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/in64_deep_2_step_mid.png}
        \caption{Two-step samples on ImageNet.}
    \end{subfigure}
    \caption{One-step and two-step samples from iCT-deep models trained on CIFAR-10 and ImageNet $64\times 64$ respectively. All corresponding samples are generated from the same initial noise vector.}\label{fig:samples}
\end{figure}

Combining all the improved techniques from \cref{sec:improvements,sec:noema,sec:pseudo_huber,sec:discretization,sec:noise}, we employ CT to train several consistency models on CIFAR-10 and ImageNet $64\times 64$ and benchmark their performance with competing methods in the literature. We evaluate sample quality using FID \citep{heusel2017gans}, Inception score \citep{SalimansGZCRCC16}, and Precision/Recall \citep{kynkaanniemi2019improved}. For best performance, we use a larger batch size and an increased EMA decay rate for the student network in CT across all models. The model architectures are based on Score SDE \citep{song2021scorebased} for CIFAR-10 and ADM \citep{dhariwal2021diffusion} for ImageNet $64\times 64$. We also explore deeper variants of these architectures by doubling the model depth. We call our method \textbf{iCT} which stands for ``improved consistency training'', and the deeper variants \textbf{iCT-deep}. We summarize our results in \cref{tab:cifar-10,tab:imagenet-64} and provide uncurated samples from iCT-deep in \cref{fig:samples}. More details and results can be found in \cref{app:exp}.

We summarize our results and compare them to previous methods in \cref{tab:cifar-10,tab:imagenet-64}. Here we exclude methods based on FastGAN \citep{liu2020towards,sauer2021projected} or StyleGAN-XL \citep{sauer2022stylegan} from our consideration, because both utilize ImageNet pre-trained feature extractors in their discriminators. As noted by \citet{kynkanniemi2023the}, this can skew FIDs and lead to inflated sample quality.

Several key observations emerge from \cref{tab:cifar-10,tab:imagenet-64}. First, iCT methods \emph{surpass previous diffusion distillation approaches in both one-step and two-step generation} on CIFAR-10 and ImageNet $64\times 64$, all while circumventing the need for training diffusion models. Secondly, iCT models demonstrate sample quality comparable to many leading generative models, including diffusion models and GANs. For instance, with one-step generation, iCT-deep obtains FIDs of 2.51 and 3.25 for CIFAR-10 and ImageNet respectively, whereas DDPMs \citep{ho2020denoising} necessitate thousands of sampling steps to reach FIDs of 3.17 and 11.0 (result taken from \citet{gu2023boot}) on both datasets. The one-step FID for iCT already exceeds that of StyleGAN-ADA \citep{karras2020analyzing} on CIFAR-10, and that of BigGAN-deep \citep{brock2018large} on ImageNet $64\times 64$, let alone iCT-deep models. For two-step generation, iCT-deep records an FID of 2.24, matching Score SDE in \citet{song2021scorebased}, a diffusion model with an identical architecture but demands 2000 sampling steps for an FID of 2.20. Lastly, iCT methods show improved recall than CT (LPIPS) in \citet{song2023consistency} and BigGAN-deep, indicating better diversity and superior mode coverage.

