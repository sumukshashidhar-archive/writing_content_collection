\section{Diffusion Models}\label{sec:diffusion}
Consistency models are heavily inspired by the theory of continuous-time diffusion models \citep{song2021scorebased,karras2022edm}. Diffusion models generate data by progressively perturbing data to noise via Gaussian perturbations, then creating samples from noise via sequential denoising steps. Let $p_\text{data}(\rvx)$ denote the data distribution. Diffusion models start by diffusing $p_\text{data}(\rvx)$ with a stochastic differential equation (SDE) \citep{song2021scorebased}
\begin{align}
    \ud \rvx_t = \bm{\mu}(\rvx_t, t) \ud t + \sigma(t)\ud \rvw_t,\label{eq:sde}
\end{align}
where $t\in[0, T]$, $T>0$ is a fixed constant, $\bm{\mu}(\cdot, \cdot)$ and $\sigma(\cdot)$ are the drift and diffusion coefficients respectively, and $\{\rvw_t\}_{t\in[0,T]}$ denotes the standard Brownian motion. We denote the distribution of $\rvx_t$ as $p_t(\rvx)$ and as a result $p_0(\rvx) \equiv p_\text{data}(\rvx)$. A remarkable property of this SDE is the existence of
an ordinary differential equation (ODE), dubbed the \emph{Probability Flow (PF) ODE} by \citet{song2021scorebased}, whose solution trajectories sampled at $t$ are distributed according to $p_t(\rvx)$:
\begin{align}
    \ud \rvx_t = \left[\bm{\mu}(\rvx_t, t) - \frac{1}{2} \sigma(t)^2 \nabla \log p_t(\rvx_t)\right] \ud t. \label{eq:pfode}
\end{align}
Here $\nabla \log p_t(\rvx)$ is the \emph{score function} of $p_t(\rvx)$; hence diffusion models are also known as \emph{score-based generative models} \citep{song2019generative,song2020improved,song2021scorebased}.

Typically, the SDE in \cref{eq:sde} is designed such that $p_T(\rvx)$ is close to a tractable Gaussian distribution $\pi(\rvx)$. We hereafter adopt the settings in \citet{karras2022edm}, where $\bm{\mu}(\rvx, t) = \bm{0}$ and $\sigma(t) = \sqrt{2t}$. In this case, we have $p_t(\rvx) = p_\text{data}(\rvx) \otimes \mcal{N}(\bm{0}, t^2 \mI)$, where $\otimes$ denotes the convolution operation, and $\pi(\rvx) = \mcal{N}(\bm{0}, T^2\mI)$. For sampling, we first train a \emph{score model} $\vs_\vphi(\rvx, t) \approx \nabla \log p_t(\rvx)$ via \emph{score matching} \citep{hyvarinen2005estimation,vincent2011connection,song2019sliced,song2019generative,ho2020denoising}, then plug it into \cref{eq:pfode} to obtain an empirical estimate of the PF ODE, which takes the form of
\begin{align}
    \frac{\ud \rvx_t}{\ud t} = -t \vs_\vphi(\rvx_t, t). \label{eq:e_pfode}
\end{align}
We call \cref{eq:e_pfode} the \emph{empirical PF ODE}. Next, we sample $\hat{\rvx}_T \sim \pi = \mcal{N}(\bm{0}, T^2 \mI)$ to initialize the empirical PF ODE and solve it backwards in time with any numerical ODE solver, such as Euler \citep{song2020denoising,song2021scorebased} and Heun solvers \citep{karras2022edm}, to obtain the solution trajectory $\{\hat{\rvx}_t\}_{t\in[0,T]}$. The resulting $\hat{\rvx}_0$ can then be viewed as an approximate sample from the data distribution $p_\text{data}(\rvx)$. To avoid numerical instability, one typically stops the solver at $t=\epsilon$, where $\epsilon$ is a fixed small positive number, and accepts $\hat{\rvx}_{\epsilon}$ as the approximate sample. Following \citet{karras2022edm}, we rescale image pixel values to $[-1,1]$, and set $T=80, \epsilon=0.002$.

Diffusion models are bottlenecked by their slow sampling speed. Clearly, using ODE solvers for sampling requires iterative evaluations of the score model $\vs_\vphi(\rvx, t)$, which is computationally costly. Existing methods for fast sampling include faster numerical ODE solvers \cite{song2020denoising,zhang2022fast,lu2022dpm,dockhorn2022genie}, and distillation techniques \cite{luhman2021knowledge,salimans2022progressive,meng2022distillation,zheng2022fast}. However, ODE solvers still need more than 10 evaluation steps to generate competitive samples. Most distillation methods like \citet{luhman2021knowledge} and \citet{zheng2022fast} rely on collecting a large dataset of samples from the diffusion model prior to distillation, which itself is computationally expensive. To our best knowledge, the only distillation approach that does not suffer from this drawback is progressive distillation (PD, \citet{salimans2022progressive}), with which we compare consistency models extensively in our experiments.
