\section{Introduction}\label{sec:intro}
\definecolor{model}{rgb}{0.7098,0.2235,0.2549}
\definecolor{ode}{rgb}{0.7216,0.8078,0.5569}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/scheme.jpg}
    \caption{Given a {\color{ode} Probability Flow (PF) ODE} that smoothly converts data to noise, we learn to map any point (\eg, $\rvx_t$, $\rvx_{t'}$, and $\rvx_T$) on the ODE trajectory to its origin (\eg, $\rvx_0$) for generative modeling. Models of these mappings are called {\color{model}consistency models}, as their outputs are trained to be consistent for points on the same trajectory.}
    \label{fig:scheme}
\end{figure}

Diffusion models \cite{sohl2015deep,song2019generative,song2020improved,ho2020denoising,song2021scorebased}, also known as score-based generative models, have achieved unprecedented success across multiple fields, including image generation \cite{dhariwal2021diffusion,nichol2021glide,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high}, audio synthesis \cite{kong2020diffwave,chen2021wavegrad,popov2021grad}, and video generation \cite{ho2022video,ho2022imagen}. %
A key feature of diffusion models is the iterative sampling process which progressively removes noise from random initial vectors. This iterative process provides a flexible trade-off of compute and sample quality, as using extra compute for more iterations usually yields samples of better quality. It is also the crux of many zero-shot data editing capabilities of diffusion models, enabling them to solve challenging inverse problems ranging from image inpainting, colorization, stroke-guided image editing, to Computed Tomography and Magnetic Resonance Imaging \cite{song2019generative,song2021scorebased,song2021medical,song2023pseudoinverseguided,kawar2021snips,kawar2022denoising,chung2023diffusion,meng2021sdedit}. However, compared to single-step generative models like GANs \cite{goodfellow2014generative}, VAEs \cite{kingma2013auto,rezende2014stochastic}, or normalizing flows \cite{dinh2014nice,dinh2016density,kingma2018glow}, the iterative generation procedure of diffusion models typically requires 10--2000 times more compute for sample generation \cite{song2020improved,ho2020denoising,song2021scorebased,zhang2022fast,lu2022dpm}, causing slow inference and limited real-time applications.




Our objective is to create generative models that facilitate efficient, single-step generation without sacrificing important advantages of iterative sampling, such as trading compute for sample quality when necessary, as well as performing zero-shot data editing tasks. As illustrated in \cref{fig:scheme}, we build on top of the probability flow (PF) ordinary differential equation (ODE) in continuous-time diffusion models \cite{song2021scorebased}, whose trajectories smoothly transition the data distribution into a tractable noise distribution. We propose to learn a model that maps any point at any time step to the trajectory's starting point. A notable property of our model is self-consistency: \emph{points on the same trajectory map to the same initial point}. We therefore refer to such models as \textbf{consistency models}. Consistency models allow us to generate data samples (initial points of ODE trajectories, \eg, $\rvx_0$ in \cref{fig:scheme}) by converting random noise vectors (endpoints of ODE trajectories, \eg, $\rvx_T$ in \cref{fig:scheme}) with only one network evaluation. Importantly, by chaining the outputs of consistency models at multiple time steps, we can improve sample quality and perform zero-shot data editing at the cost of more compute, similar to what iterative sampling enables for diffusion models.


To train a consistency model, we offer two methods based on enforcing the self-consistency property. The first method relies on using numerical ODE solvers and a pre-trained diffusion model to generate pairs of adjacent points on a PF ODE trajectory. By minimizing the difference between model outputs for these pairs, we can effectively distill a diffusion model into a consistency model, which allows generating high-quality samples with one network evaluation. By contrast, our second method eliminates the need for a pre-trained diffusion model altogether, allowing us to train a consistency model in isolation. This approach situates consistency models as an independent family of generative models. Importantly, neither approach necessitates adversarial training, and they both place minor constraints on the architecture, allowing the use of flexible neural networks for parameterizing consistency models.

We demonstrate the efficacy of consistency models on several image datasets, including CIFAR-10 \cite{krizhevsky2009learning}, ImageNet $64\times 64$ \cite{deng2009imagenet}, and LSUN $256\times 256$ \cite{yu2015lsun}. Empirically, we observe that as a distillation approach, consistency models outperform existing diffusion distillation methods like progressive distillation \cite{salimans2022progressive} across a variety of datasets in few-step generation: On CIFAR-10, consistency models reach new state-of-the-art FIDs of 3.55 and 2.93 for one-step and two-step generation; on ImageNet $64\times 64$, it achieves record-breaking FIDs of 6.20 and 4.70 with one and two network evaluations respectively. When trained as standalone generative models, consistency models can match or surpass the quality of one-step samples from progressive distillation, despite having no access to pre-trained diffusion models. They are also able to outperform many GANs, and existing non-adversarial, single-step generative models across multiple datasets. Furthermore, we show that consistency models can be used to perform a wide range of zero-shot data editing tasks, including image denoising, interpolation, inpainting, colorization, super-resolution, and stroke-guided image editing (SDEdit, \citet{meng2021sdedit}).




