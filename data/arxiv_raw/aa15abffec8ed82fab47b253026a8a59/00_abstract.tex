Atmospheric states derived from reanalysis comprise a substantial portion of weather and climate simulation outputs. Many stakeholders --- such as researchers, policy makers, and insurers --- use this data to better understand the earth system and guide policy decisions. Atmospheric states have also received increased interest as machine learning approaches to weather prediction have shown promising results. A key issue for all audiences is that dense time series of these high-dimensional states comprise an enormous amount of data, precluding all but the most well resourced groups from accessing and using historical data and future projections. To address this problem, we propose a method for compressing atmospheric states using methods from the neural network literature, adapting spherical data to processing by conventional neural architectures through the use of the area-preserving HEALPix projection.
We investigate two model classes for building neural compressors: the hyperprior model from the neural image compression literature and recent vector-quantised models.
We show that both families of models satisfy the desiderata of small average error, a small number of high-error reconstructed pixels, faithful reproduction of extreme events such as hurricanes and heatwaves, preservation of the spectral power distribution across spatial scales.
%High-performance multi-decadal atmospheric compression methods have up to this point been missing in the field.
We demonstrate compression ratios in excess of $1000\times$, with compression and decompression at a rate of approximately one second per global atmospheric state.


%%%% RECENT COMMENTS

% These problems will be exacerbated by increasing the resolution, the number of years, and the number of scenarios of Earth System Models. %higher-resolution climate projections, where 100-year data of different scenarios and models increase the amount to data. 

%such that a larger audience can access and process such data efficiently.

%(MAE around 0.4$^\circ$~K for temperature, around $0.5~\text{m}/\text{s}$ for zonal and meridional wind, below 100 Pa for surface pressure and around $40~\text{m}^2/\text{s}^2$ for geopotential), 

%To our knowledge, this is the first work that studies neural compression at a multi-decadal scale with a high compression rate of $1000\times$ and above, and that aims at releasing the compressed outputs for use by the community.\todo{CRA5? Also, under-promise and over-deliver?}
%We provide our open source code and compressed outputs for use by the community.\dwf{Should this line be in the initial arXiv version if the release is not simultaneous?}


%%%%% THIS IS OLD

%Atmospheric data comprise a large percentage of the output generated by both weather and earth system (climate) models. Such data have been important historically to researchers study of effects of dynamics , but has also received increased researcher attention %have now also viewed this data with an increasing level of interest as 
%for training machine-learning surrogates of atmospheric dynamical models. % A key component of these surrogate models
% are approaching the quality of traditional simulation systems
% are the large corpora on which they are trained. 
%Such corpora, however, pose significant engineering challenges as the data are extremely large
%One challenge for using these datasets are simply its size: for instance, 40 years of 0.25 degree data at 1 hour timesteps requires 250TB of storage. Such requirements preclude all but the most well resourced groups to tackle such problems. In fact, even transferring such data incurs a significant cost.

%We aim to lower the barrier to entry by learning compressed representations of atmospheric states, with the goal of lowering storage needs, more quickly transferring data, and perhaps reducing the accelerator requirements for learning ML surrogates. We investigate two classes of models -- the hyperprior and VQ-VAE/GAN -- for neural compression, with the desiderata that: 1) the average error is small, 2) the maximum error is bounded, 3) error on extreme values is limited, and 4) spectral properties are preserved. We find that, with the appropriate choice of grids and models, that we are able to compress atmospheric data that satisfy these desiderata by a factor of 1000X \emph{obv, this number should change, and the writing is weird}. Further, we open-source the code for use by the community, and store compressed representations of ERA5 data to enable fast transfer. Moreover, to the best of our knowledge, this is the first large-scale study on neural compression on a large corpus of high-resolution data.