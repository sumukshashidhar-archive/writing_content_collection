\subsubsection{VQ-VAE and VQ-GAN}

The vector-quantized variational autoencoder, or VQ-VAE~\citep{oord2017vqvae} is a neural network that maps input data to a set of discrete indices into a learned codebook of vector-valued codes.
Taking inspiration from \cite{kingma2014vae}, a continuous neural encoder maps the input $x$ to a continuous code layer. While the original VAE relied on amortized inference (by the encoder) of sufficient statistics of continuous random variables and a ``reparameterization trick'' to obtain differentiable samples~\citep{williams1992simple,rezende2014reparameterizationtrick}, the VQ-VAE defines a degenerate variational posterior in which (components of) the latent representation adopt the closest (in Euclidean distance) of $N$ distinct values from a learned codebook~$D$.
In the case of images, convolutional architectures~\citep{lecun1989generalization} are employed for the encoder and decoder, and a vector $z_{ij}$ at each spatial location in the lower dimensional, final layer spatial map is quantized to values from a codebook shared across all spatial locations. The encoder and decoder are trained end-to-end by minimizing the L2 norm between the input data and their reconstructions. During this optimization, the gradients through the non-differentiable quantizing operation are estimated by simply ignoring it and substituting the identity function~\citep{hinton2012coursera,bengio2013estimating,theis2017lossy}.
Additional loss functions steer the codebook entries toward the output of the encoder while also encouraging the encoder to ``pick'' between codebook entries rather than predict convex combinations thereof (see \citet{oord2017vqvae} for details).
%Like a VAE~\citep{kingma2014vae}, it uses a continuous neural encoder to map the input $x$ to continuous-valued means~$\sigma$ and variances~$\mu$ of a set of independent scalar normal distributions. The decoder samples from these distributions to get a latent vector~$z$, which it then feeds through a decoder network to generate an approximation of the input $x'$. The encoder-decoder pipeline is trained end-to-end by stochastically optimizing the MSE reconstruction loss $||x-x'||$. The loss gradients are pushed through the otherwise undifferentiable stochastic sampling step using the ``reparameterization trick''~\citep{kingma2014vae, rezende2014reparameterizationtrick}. This decomposes the sampling step into a differentiable transform and a stochastic noise function with zero mean and unit variance, the latter of which is taken to have no effect on the gradient (in expectation) and is therefore ignored during differentiation. 

%VQ-VAE adds vector quantization to this. Rather than let latent vector~$z$ adopt any value, it constrains it to be one of $N$ possible values from a learned codebook~$D$. 
Once quantized, an encoded value can be parsimoniously represented as an integer index $i \in [1, N]$ such that $z \approx D_i$.
Beyond the compression that results from replacing a multi-channel image with a much smaller array of integers, the resulting map can be further compressed with standard entropy-coding techniques, potentially exploiting both non-uniformity of codebook usage and spatial correlations in the resulting low-dimensional maps. Figure~\ref{fig:vqvae} illustrates the VQ-VAE network, depicting the encoder/decoder and the codebook reconstruction.

As noted in section~\ref{sx:introduction}, na\"ively optimizing the mean squared error can have unacceptable consequences for atmospheric states, as its tendency to wash out sharp features can erase the very events climate scientists wish to measure, such as hurricanes.
We therefore also investigate VQ-GANs~\citep{esser2021taming}, which augment a VQ-VAE by adding a patch-wise discriminator~\citep{isola2017image} and are trained with an auxiliary adversarial loss term~\citep{goodfellow2014gan}. During training, the discriminator receives the quantized feature map indices, channel-wise concatenated (after upsampling and continuous embedding) with either the original atmospheric data from which it was computed or its corresponding reconstruction. The discriminator is trained to maximize its classification accuracy on discriminating patches of real data from reconstruction patches, while the encoder-decoder pipeline is trained to reconstruct its input well while minimizing this accuracy.
Adversarial losses were similarly used for restoring high spatial frequencies for precipitation nowcasting in~\cite{ravuri2021nowcasting}.
For both VQ-VAE and VQ-GAN experiments, we base our neural network architectures on those presented in \cite{esser2021taming}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/networks/vqgan.pdf}
    \caption{The VQ-GAN network, which contains a VQ-VAE network and an auxiliary discriminator. The encoder and decoder are residual convnets with attention, which reduce the input data into an 18x18 grid of vectors of size 1024. These are then quantized to their nearest match in the learned codebook of N vectors. See Appendix~\ref{sx:appendix:models} for details.}
    \label{fig:vqvae}
\end{figure}

\subsubsection{Factorized prior and Hyperprior Models}

Whereas VQ models discretize a latent vector $z$ to its nearest neighbor in a learned codebook, the factorized prior model~\citep{balle2017fp} discretizes individual elements of $z$ by rounding to integers at test time.
This is approximated during training as a zero-mean scalar noising function in the same spirit as the ``reparameterization trick'' described above.
Unlike the VQ models described earlier, the factorized prior's objective function is explicitly derived with data compression in mind, and a continuous relaxation of the Shannon entropy of the resulting discrete code comprises an additional loss term.
Probabilistically speaking, the encoder-decoder pair are trained to optimize a variational lower bound on the expected log-likelihood of the data modeled by latent variables with a prior consisting of independent uniform random variables (hence, ``factorized'') and an isotropic Gaussian likelihood.
%This noise function is also sampled from during decoding, to convert integers to continuous values.

The hyperprior model~\citep{balle2018hp} is an extension which, for training purposes, composes two factorized prior autoencoders.
The first block's encoder produces a feature map~$z$, which is quantized to integers~$\hat z$.
As is common in convolutional feature maps, the pixels of~$\hat{z}$ retain local spatial correlations.
The second autoencoder implements, with the same variational approach (though a different likelihood function) a probabilistic generative model which captures these correlations, introducing a new, smaller set of latent variables $w$\footnote{\citet{balle2018hp} uses the notation $y$ and $z$, rather than $z$ and $w$, for first and second level latent variables, respectively.}.
The probabilistic model defined by this secondary autoencoder is used to more effectively entropy code the first model's quantized maps $\hat{z}$ by predicting the scale (i.e.\ modeling the variance) of each element of $\hat{z}$ conditioned on the encoded, quantized secondary latent variables $\hat{w}$, which are themselves entropy-coded and stored alongside the  resulting encodings of $\hat{z}$.
Figure~\ref{fig:hyperprior} shows the network schematic for the hyperprior model, while Figure~\ref{fig:balle} gives a detailed picture including layer sizes.
We recommend the hyperprior model as the most suitable neural compressor that performs well across all the key requirements considered in the introduction.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/networks/hyperprior_horizontal.pdf}
    \caption{Hyperprior network. This augments the factorized prior network (dashed rectangle) by adding an additional encoder-decoder pathway that encodes the FP network's latents, and decodes them to a grid of variances.}
    \label{fig:hyperprior}
\end{figure}



\subsubsection{Effective compression ratio}

The na\"ive compression ratio for the VQ-VAE or VQ-GAN model would consist in dividing the storage space needed for representing input data (e.g., 5 vertical variables times $256 \times 256$ pixels of 32-bit floating point data) vs. the storage space needed by the compressed vector-quantized maps (e.g., for a VQ-VAE with 3 downsampling blocks, $32 \times 32$ indices of a 13-bit dictionary with $2^{13} = 8192$ elements)\footnote{Given that we use overset HEALPix grids detailed in Appendix \ref{sx:appendix:overset_healpix}, the compression is computed as a ratio between $288 \times 288$ 5-channel 32-bit images and $n$-bit $36 \times 36$ maps.}. For this specific example, the compression ratio is about 788. In entropy coding can further reduce the storage footprint, we estimate the compression ratio to be approximately $1100\times$ for that model architecture when compressing test data.

For the purpose of this work, we estimate storage cost in bits as the empirical Shannon entropy of codebook indices per grid, $H = -\sum_i f_i \log_2 f_i$, where $f_i$ is the observed frequency of codebook element $i$ in a given grid. 
This notably ignores expected spatial correlations in the encoded representation (for which the hyperprior model explicitly accounts). The obtained compression ratios for VQ models could thus be improved by fitting a secondary autoregressive model on the codebook index maps\footnote{While not chiefly concerned with compression, \cite{oord2017vqvae} employ autoregressive PixelCNN models of codebook indices to lower-bound the log likelihood of test data as well as for the generation of \emph{de novo} samples.}. The compression ratios obtained with this simplistic entropy computation thus serve as a conservative lower bound.

\subsection{HEALPix map projection}

The representation and projection of the initial data that is to be compressed is a key consideration in the design of neural compression methods, and it is this first step depicted in Figure \ref{fig:system} that we now describe. 
Atmospheric data lies on a sphere, whereas numerical programming natively operates on rectilinear arrays. 
The equirectangular latitude/longitude projection suffers from fundamental limitations in that pixels do not represent consistent units of area across the map, ranging from $28~\text{km} \times 28~\text{km}$ at the equator to negligible area at either pole (where the grid heavily oversamples).
To address these limitations, there exist a multitude of alternative gridding conventions, with the most common reprojections being octahedral~\citep{smolarkiewicz2016fvm} and icosahedral~\citep{zangl2015icon} sampling, cubed spheres~\citep{lin1997cubedsphere,ronchi1996cubed}, dual lat-lon / ``Yin-Yang''~\citep{cote1998gem,ohno2009visualization} maps, and locally linear projections~\citep{metoffice2016data}.

%One workaround to this mismatch is to convert the data to spherical harmonics, a natural means of representing spherical data in a linear array of coefficients. However, reconstruction errors in these coefficients can introduce systematic artefacts across the globe. Unlike convolution-based neural networks, which are geared to encoding local features, spherical harmonics do not represent local features. Another way to avoid projections is to use a model that can represent spherical inputs as a local graph, such as GraphNets~\citep{lam2023graphcast}. However, in our context of compressing a dataset for general use, we prefer to be versatile in the choice of model, many of which use some form of reprojection from the earth's sphere to a two-dimensional array.

% From Table 3 in https://gmd.copernicus.org/preprints/gmd-2017-108/gmd-2017-108-manuscript-version6.pdf

%Researchers often jointly design their models and reprojection schemes, resulting in almost as many gridding conventions as there are models~\citep{ullrich2017review}. These include 
%HEALPix provides three key features not provided by other projections: (1) it is efficiently convertible to any arbitrary other projection and this maintains compatibility with other projections that maybe needed; (2) it naturally splits the data into grids that are of a manageable size for machine learning accelerators such as GPUs and TPUs; (3) it roughly preserves area, thus avoids the need to learn redundant features over multiple scales. %
The Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) projection~\citep{gorski1999healpix} defines a curvilinear partitioning of the sphere into 12 equal-area base pixels, further subdivided by powers of 2 into a local square coordinate system, which can be sized to the needs of the application.
The pixels of a HEALPix grid, when traversed diagonally, lie along lines of constant latitude, making conversion to and from spherical harmonics especially efficient\footnote{We have explored the use of cubed sphere \citep{ronchi1996cubed,lin1997cubedsphere} and Yin-Yang \citep{cote1998gem,ohno2009visualization} projections, that have the advantage of \emph{overset}, i.e., overlapping grids, but where grid points do not lie on equally spaced equilatitude rings that prove useful for the computation of spherical harmonics.}.
This ease of conversion to the lingua franca of spherically embedded continuous data makes HEALPix a convenient grid format for conversion to any other grid format.
HEALPix is widely used in astronomy for data situated on the celestial sphere, and is seeing increasing use in atmospheric science in classical NWP models~\citep{karlbauer2023advancing} and in deep learning-based methods~\citep{ramavajjala2024heal, chang2023seamless}. %, and discussed as a potential standard grid format for the field.

Figure \ref{fig:system} illustrates the projection of atmospheric data onto HEALPix, and specifically shows 3 such HEALPix base pixels (base pixel 7 is equatorial whereas base pixels 10 and 11 are polar). It also illustrates the forward transform of the HEALPix representations to compute spherical harmonics coefficients (analysis), and the inverse transform of spherical harmonics coefficients to compute latitude/longitude projections (synthesis). To avoid losing high-frequency information, we choose maximum wavenumber $l_{max}=721$ (equal to the number of latitudes on the latitude/longitude grid and half the number of longitudes during synthesis of the reprojection image) for the spherical harmonics transform; see Appendix~\ref{sx:appendix:spherical_harmonics} for more details. One important consideration when using HEALPix is that the round-trip from latitude/longitude representations to HEALPix projections at our chosen resolution to spherical harmonic coefficients back to latitude/longitude reprojection introduces small interpolation errors, which remove some high-frequency information (as visible in Figure~\ref{fig:spectra}, illustrating the comparison between power spectra of the ground truth and power spectra of the reprojection).

In the experimental analysis that follows, we will use HEALPix with a $256 \times 256$ pixel grid within each of the 12 top-level base pixels, preserving a spatial resolution of around 0.25 degrees per pixel. For reference, the equirectangular projection in which ERA5 data are distributed contains arrays of 721 latitudes by 1440 longitudes, i.e., $1,038,240$ pixels (with significant oversampling at the poles) whereas the HEALPix projection at $256 \times 256$ contains $786,432$ pixels; we show in Appendix \ref{sx:appendix:overset_healpix} that these HEALPix pixels cover smaller areas than lat/lon cells at the equator.
Similarly to \cite{karlbauer2023advancing}, we also extend HEALPix to support overset grids, or grids with overlapping domains at the edges. On each side of each $256 \times 256$ grid, we add a margin of 16 pixels, whose values are interpolated from the neighboring grids with which they overlap. This enables us to remove discontinuities between neighboring grids in the decompressed data, by linearly blending between the two grids' values where they overlap. See Appendix~\ref{sx:appendix:overset_healpix} for details.

%While not focused on data compression but rather on weather forecasting and climate modeling tasks, a number of recent deep learning methods tackled spherical data, from GraphCast \citep{lam2023graphcast} and GenCast \citep{price2023gencast} operating on local Graph Neural Networks, as well as Convolutional Graph Networks \citep{defferrard2019deepsphere} or Spherical Convnets \citep{cohen2018spherical}, to Spherical Fourier Neural Operators \citep{pathak2022fourcastnet,bonev2023spherical}.
